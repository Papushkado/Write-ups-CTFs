{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4adc7127",
   "metadata": {},
   "source": [
    "# Séries de challenges : Gorfoustral\n",
    "\n",
    "## (très) Rapide introduction aux outils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82afb505",
   "metadata": {},
   "source": [
    "Pour cette suite de challenge, je vous propose d'utiliser Transformer Lens : <https://transformerlensorg.github.io/TransformerLens/>, c'est une superbe librairie pour faire de la rétro-ingénierie de transformers.\n",
    "\n",
    "_Petit point important à prendre en compte, pour load un modèle, Transformer Lens va **toujours** appeler Hugging Face, même si le modèle existe déjà sur votre machine, et que le paramètre associé est à False..., c'est overwrite à True dans le code de la lib... Si c'est un problème pour vous, vous pouvez utiliser la librairie Transformers, tout est faisable avec. Si vous avez des questions, n'hésitez pas à venir me voir en DM : @sckathach / Le magicien quantique_\n",
    "\n",
    "Pour load un modèle et son tokenizer : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbd77254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-medium into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "import transformer_lens as tl\n",
    "\n",
    "model = tl.HookedTransformer.from_pretrained_no_processing(\"gpt2-medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45250f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|endoftext|>', 'Super', 'be', ' phrase', ' t', 'oute', ' coup', 'ée']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokénisation :\n",
    "model.to_str_tokens(\"Superbe phrase toute coupée\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d5ae115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50256, 12442,  1350,  9546,   256, 13192, 12092, 22161]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_tokens(\"Superbe phrase toute coupée\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d10030c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cd8bd3c013143d59eeb6825a15edfbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['<|endoftext|>Paris is the capital of France,']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Génération\n",
    "tokens = model.to_tokens(\"Paris is the capital of\")\n",
    "output = model.generate(tokens, max_new_tokens=2, temperature=0)\n",
    "model.to_string(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2eb6d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accès aux activations intermédiaires :\n",
    "logits, cache = model.run_with_cache(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce85539c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 16, 64])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Résultat des têtes à la couche 7 : (batch_size, seq_len, n_heads, d_model)\n",
    "cache[\"z7\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d017eb",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "\n",
    "Pour cette suite de challenge, le modèle - le _gorfoustral_ - a été entraîné à partir de GPT2-medium à retenir le drapeau. L'objectif est très simple, récupérer le drapeau à partir du modèle. \n",
    "\n",
    "Pour le challenge d'introduction, le modèle n'a pas subit de transformation, vous pouvez le load et le tester avec :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf0482c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-medium into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from gorfougym import load_model, test_flag\n",
    "\n",
    "gorfoustral = load_model(\"gorfoustral-1_300M.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46d4c38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3929f60beb442fa822072e366cfe2a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "' False'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_flag(gorfoustral, \"404CTF{un_super_flag!}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15d83b1",
   "metadata": {},
   "source": [
    "***Rappel important : tous les drapeaux sont sous la forme `404CTF{une_phrase_tres_simple_avec_des_underscores_entre_les_mots}`. Ce sera important pour flag, par exemple, si votre méthode n'est pas suffisament précise et si vous trouvez la séquence `gorfoustrX e...`, essayez `gorfoustral_...`. Ce sera sûrement le cas pour le challenge 3, n'hésitez pas à venir me voir en DM si vous pensez avoir la solution. Les challenges sont callibrés pour avoir maximum 3, 4 choix à faire, avec le contexte de la phrase, cela ne doit pas poser problème.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4441a6ac",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vents2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
